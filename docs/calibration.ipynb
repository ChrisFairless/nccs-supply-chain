{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calibration of the NCCS pipeline\n",
    "\n",
    "This document goes through some of the background "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "//What needs calibrating?\n",
    "\n",
    "//What are our observations?\n",
    "\n",
    "//What does our approach need to do?\n",
    "\n",
    "//What solution did we pick?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methodology\n",
    "\n",
    "## An example\n",
    "\n",
    "Let's choose a concrete example that we can follow through this workbook. We'll start with something fairly simple: a two-dimensional parameter space describing asset losses for tropical cyclones.\n",
    "\n",
    "//Describe example: calibrating TC impact function\n",
    "//Show the parameter space for an impact function\n",
    "\n",
    "## What does a Bayesian approach mean?\n",
    "\n",
    "//Bayesian versus frequentist\n",
    "\n",
    "//Advantages of a Bayesian approach\n",
    "\n",
    "## What is an optimisation?\n",
    "\n",
    "An algorithmic optimisation lets us search a parameter space for the combination of values that best 'solves' some problem that we're interested in. This could be as simple as a linear regression, or as complex as tuning the hyperparemeters of a neural network. In our case we're interested in tuning impact functions in CLIMADA, adjusting their size and shape until the modelled losses best resemble the observations that we have.\n",
    "\n",
    "Every optimisation problem requires three elements (or some variation on these three elements):\n",
    "\n",
    "- **The parameter space:** information about all the different variables whose possible values we want to explore, plus any constraints we want to place on their permitted values, or rules about which combinations of values can be used. In this case, these are the parameters that together define our impact function(s).\n",
    "- **The objective function:** a function (often a statistical model) that takes these parameters as input and maps them to some output. For us, this is a wrapper around part or all of our modelling chain that maps the input parameters to various modelled impacts. In the example this uses the input parameters to create an impact function, and runs CLIMADA with this impact function, outputting losses. The output doesn't have to be a single value, it can be anything you like: for example, average annual loss from an event set, or a vector of losses from every event on every sector, or losses from a single location, or a combination of all these things. \n",
    "- **The cost function:** This takes two inputs, the output from the objective function, and someobservational or training data which is considered to be the target value(s) for the objective function. The cost function tells the optimisation how well or poorly each combination of parameters performs compared to the observations. In our example, our observations are event losses from EM-DAT and the cost function calculates the difference between modelled and observed losses and returns a summary statistic (we discuss the choice of this statistic later).\n",
    "\n",
    "The optimisation then explores the parameter space, attempting to find a combination of parameters that minimises the cost function (and in our case, in as few steps as possible). Most of the content below ends up being about this process to find the combination of parameters that gives the minimum value of the cost function: how to model an approximation of the loss function and how to sample from the parameter space most efficiently.\n",
    "\n",
    "An optimisation algorithm searches for this the optimal parameter combination that minimises the cost function with an iterative process. For each iteration:\n",
    "\n",
    "- The algorithm looks at the points it has already sampled from the parameter space and what it knows about the behaviour of the cost function\n",
    "- It uses some modelling process (see below) to choose another point in the parameter space to sample that will add the most useful information about the location of the function's minimum (discussed below too)\n",
    "- It runs the model (the objective function) for the new sampled parameters from the parameter space and calculates the value of the cost function\n",
    "- If some stopping criterion is met (e.g. only small improvements in our knowledge of the cost function, a maximum number of iterations reached) it stops. Otherwise it starts the next iteration.\n",
    "\n",
    "This process can be represented as a flow chart:\n",
    "\n",
    "<img src=\"images/calibration_optimisation_flow.png\" width=\"500\" alt=\"Flow chart representing the flow of an optimisation algorithm\">\n",
    "\n",
    "How the algorithm thinks about the form of the cost function and how it chooses its next sample from the parameter space are big and interesting problems with a lot of solutions. The solution we will choosing is Bayesian optimisation with Gaussian process priors. The Bayesian optimisation is our approach for choosing the next point, the Gaussian process priors refer to how we approximate our the cost function with what we know.\n",
    "\n",
    "\n",
    "## Why do we use Gaussian processes in optimisation?\n",
    "\n",
    "A Gaussian process is a good way to approximate any unknown function defined over a continuous domain where we know its value at a finite set of points (e.g. the value of the cost function). In particular, the way a Gaussian process approximates a function is mathematically nice: as awe get more information the approximation gets more and more accurate, it doesn't expect a particular statistical distribution or functional form (which we wouldn't expect in a complex problem like this) and it has, predictable, cheap-to-calculate properties, including marginals and conditionals which are important in Bayesian inference. These is important when we use the approximated function for operations such as choosing the next parameter combination to test.\n",
    "\n",
    "In straightforward approaches to an optimisation with a Gaussian process, the optimising algorithm will gradually refine its understanding of the parameter space by sampling points from within it. The optimiser will typically look at the local gradient of the function and its Hessian (i.e. second derivatives) to choose the next location to sample. This works well, but there are more efficient ways to search if you really want to reduce the number of times you have to call your objective function (the model)\n",
    "\n",
    "At the cost of more expensive calculations of where to sample next, a Bayesian process can make better inferences about the combination of parameters that, when sampled, are most likely to improve its knowledge about the cost function's minimum.\n",
    "\n",
    "\n",
    "## What is Bayesian optimisation?\n",
    "\n",
    "Bayesian optimisation refers to a particular family of optimisation algorithms that considers the objective function itself as probabilistic, that is, each step of the optimisation isn't considering a single 'best guess', but instead looks at the whole space of possible approximating functions (in our case, the space of Gaussian processes defined on samples from our parameter space). Each function in the space has an associated plausibility quantifying how well it does (or doesn't) agree with the data we have.\n",
    "\n",
    "A Bayesian optimisation algorithm consists of an iterative process that repeatedly (and very very strategically) samples the parameter space based on what it already knows about the cost function's behaviour (the prior distribution), runs our model for the sampled parameters and calculates the cost function, and then updates its statistical description of the plausibility of all approximating functions with the new information. This updated description is the 'posterior' distribution. Each step of the optimisation algorithm is mathematically a Bayesian update, the core process of all Bayesian inference.\n",
    "\n",
    "\n",
    "## Why Bayesian optimisation is pretty useful\n",
    "\n",
    "We choose Bayesian optimisation because it prioritises reaching conclusions in _as few steps as possible_ of the optimisation iteration. Given how expensive it is to run our modelling chain (or even a component of the chain) we want to minimise the number of times we have to run the model. This isn't always a requirement of optimisation routines: many (even most) statistical models are fairly simple and create objective functions that can be evaluated in microseconds. In these cases the model's computational cost isn't a huge factor in algorithm design. Thankfully, a lot of people care about exactly this problem at the moment because a lot of people are exploring hyperparemeter spaces used to design machine learning models, which are usually quite expensive to train.\n",
    "\n",
    "Bayesian optimisation does this by allowing our algorithm to consider more information (encoded as uncertainty) in its decision-making than other, simpler approaches. Each iteration of the optimisation algorithm is a bigger improvement on the previous step when compared with the stepwise improvements that other algorithms make (usually! It of course depends on you setting up your model and optimisation correctly). Note: a Bayesian approach isn't the only way to accomplish this! But it ends up being quite a neat one, in my opinion at least.\n",
    "\n",
    "This comes at the cost of more computationally complex overhead to decide on the next sample, but when our model is expensive to run this is definitely worth it.\n",
    "\n",
    "\n",
    "## The posterior distribution\n",
    "\n",
    "Describing the cost function as a probabilistic family of functions has other benefits. It means that the algorithm's final output telling us what it has inferred about the cost function's form and the location of its minimum isn't just a single point estimate from the parameter space, we get a full posterior distribution of cost function approximations that we can sample to explore the full range of plausible impact functions generated through the calibration. \n",
    "\n",
    "\n",
    "## What is an acquisition function?\n",
    "\n",
    "//It's another cost function. This one is used to choose the next combination of parameters to sample from the parameter space.\n",
    "\n",
    "\n",
    "## The algorithm\n",
    "\n",
    "In this Bayesian formulation of the problem, each step of the iterative search proceeds as in the above outline:\n",
    "\n",
    "- We want to approximate the cost function with a Gaussian process. The Bayesian approach assumes that there is a non-parametric statistical distribution of cost functions, each with a prior plausibility. If this is the first iteration, the priors are set by the user. Otherwise the posterior of the previous iteration becomes the prior for the next iteration.\n",
    "- An aquisition function (see above) is used to choose a new point to sample from the parameter space, trying to maximise the additional information we gain from evaluating the objective function at this point.\n",
    "- The model is run and the cost function calculated\n",
    "- Our probabilistic model of plausible approximations of the objective function is updated with this new data point to generate a posterior distribution of plausible approximations of the objective function. This approximation will be more accurate and constrained than the prior.\n",
    "- If some user-provided stopping criterion is met it stops. Otherwise it starts the next iteration.\n",
    "\n",
    "The optimisation module underlying all our code uses this approach and is described in the documentation for the `bayesian-optimisation` package: https://github.com/bayesian-optimization/BayesianOptimization.\n",
    "\n",
    "A wrapper around the algorithm has been produced by Lukas Riedel and is in CLIMADA's `calibration` package. Many many thanks to Lukas!\n",
    "\n",
    "For more information on Bayesian optimisation in general see https://proceedings.neurips.cc/paper_files/paper/2012/file/05311655a15b75fab86956663e1819cd-Paper.pdf\n",
    "\n",
    "\n",
    "## Summary: why Bayesian?\n",
    "\n",
    "- It lets us use all available info without making further assumptions\n",
    "- It works with probability distributions of how plausible different descriptions of our the objective function are, given the samples it has made.\n",
    "- ... this distribution is _nonparametric_, i.e. it can have an arbitrary and complex functional form, which we would require for a complex multidimensional parameter space like the one we're exploring\n",
    "- ... and this posterior probability distribution can be sampled as a way of exploring model uncertainty\n",
    "\n",
    "\n",
    "\n",
    "## How we calibrate a component\n",
    "\n",
    "//Model description\n",
    "\n",
    "## How we calibrate the whole pipeline\n",
    "\n",
    "//description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A note on uncertainty simulations\n",
    "\n",
    "One of the lovely things about (a set of) Bayesian posteriors (and also priors) is that they are 'generative'. That means that you can sample from it, as with parametric distributions. This allows us to think of parameter we've calibrated as uncertain, not just a point estimate.\n",
    "\n",
    "Until now we've just been running model chains using the 'best' point estimates for each parameter. But we can also run simulations with other choices of each parameter. In theory, we could run hundreds of supply chain simulations, each one sampling from the uncertainty distribution of the calibrated parameters. This will give a 'full' range of uncertainty in the modelled results, where the highest impact in the output simulations isn't just from the most intense event in the event set, it's from the most intense event _in the simulation that sampled the most extreme, unlikely impact function_.\n",
    "\n",
    "CLIMADA's `unsequa` module provides a suite of tools to set up these simulations.\n",
    "\n",
    "## Storylines\n",
    "\n",
    "In practice, however, we may decide that these are far too expensive computationally. Currently a full modelling chain takes many many hours to run. While we have a few ideas on how to speed it up (see below), and there are some neat ways to explore parts of the parameter uncertainty space without recalculating all of the impacts, we may decide that we don't have the time or computational resources for a fully probabilistic exploration of the uncertainty space.\n",
    "\n",
    "If this happens, we would fall back on the well-loved Storylines approach\n",
    "//explain storylines\n",
    "\n",
    "\n",
    "## Simulations to explore uncertainty\n",
    "To save computation time, we probably want to look at the uncertainty around individual componenents of the modelling chain. That is, look at one hazard at a time, and maybe look at uncertainty in direct asset loss and business interruption separately. While this will miss a lot of interaction effects, it's very good for telling a clear story, and it's also good for easier-to-analyse results!\n",
    "\n",
    "//more?\n",
    "\n",
    "## How to speed up simulations\n",
    "Most readers should skip this section. It's a set of notes, pretty technical, and not required for a full understanding of the modelling!\n",
    "\n",
    "- Deployment on the CelsiusPro VM. This has a _lot_ of computational resources.\n",
    "- Component-wise uncertainty assessment, as described in the above section.\n",
    "- Linear scaling of direct impacts is cheap //explain\n",
    "- Collapsing the event set: //explain \n",
    "    - removing years with similar impacts //explain\n",
    "    - removing years with lower impacts //explain\n",
    "- Reducing IO costs (mostly the need to read in Exposures multiple times and to store an Impact object's `imp_mat` attribute. Both of these could be avoided after Samuel Juhel's refactor of the Supply Chain module, we hope).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calibration implementation\n",
    "\n",
    "The calibration is being implemented in four Versions, each one improving on the previous. Version 1 starts with a simple regional calibration of impact functions against observations by introducing either more explanatory variables to fit, or by fitting a more complex pipeline.\n",
    "\n",
    "The first th\n",
    "\n",
    "## Version 1: calibrate direct asset loss impact functions\n",
    "\n",
    "Here we take a very pragmatic approach. The goal is to calibrate everything component-wise, and to calibrate only the components where there is enough data for a meaningful level of certainty in the outputs.\n",
    "\n",
    "In practice, that means calibrating direct asset loss impact functions for each hazard to EM-DAT loss data. In this first version we don't have enough data to meaningfully improve on the HAZUS functions in the business interruption components.\n",
    "\n",
    "Therefore, we are calibrating this part of the modelling pipeline:\n",
    "\n",
    "<img src=\"images/calibration_component_asset_loss.png\" width=\"800\" alt=\"The NCCS modelling pipeline with the asset loss component highlighted\">\n",
    "\n",
    "\n",
    "Zooming in on the calibrated component, the detailed optimisation looks like this:\n",
    "\n",
    "<img src=\"images/calibration_component_asset_loss_detailed.png\" width=\"500\" alt=\"A flow chart for calibration against asset loss\">\n",
    "\n",
    "\n",
    "### Choice of impact function and parameter space\n",
    "\n",
    "For each of the impact functions we're working with, we assume that the impact function we're fitting is a sigmoid function. This is very common in risk modelling.\n",
    "\n",
    "A typical sigmoid curve in CLIMADA looks like this (this is the (old) out-of-the-box impact function for asset damage from tropical cyclone winds):\n",
    "\n",
    "<img src=\"images/calibration_emanuel_sigmoid.webp\" width=\"500\" alt=\"CLIMADA's default impact function sigmoid curve for TC asset losses\">\n",
    "\n",
    "A sigmoid function is defined (for us) by three variables, `v_thresh`, `v_half` and `scale`:\n",
    "\n",
    "- `v_thresh` is the point on the x-axis where impacts start\n",
    "- `v_half` is the point on the x-axis where impacts reach 50% of `scale`\n",
    "- `scale` is the maximum value of an impact (usually 100%)\n",
    "\n",
    "Adjusting `v_thresh`and `v_half` together is equivalent to a translation of the function.\n",
    "\n",
    "Adjusting `v_thresh` is equivalent to stretching the function along the x-axis (with `v_half` held constant).\n",
    "\n",
    "Adjusting `scale` is equivalent to a vertical scaling of the function.\n",
    "\n",
    "The most basic restrictions on the parameters are that they must all be positive, and `v_half` must always be larger than `v_thresh`. In practice we use more restrictive priors to narrow the search space.\n",
    "\n",
    "\n",
    "### Choice of cost function\n",
    "\n",
    "The choice of the cost function here is important, because it determines what we want to get right. There are two common choices in risk modelling like this: either the mean square difference or the mean square log difference. Both of these are common error statistics. The first is a measure of how far off you are when you're trying to reproduce each observation. The second tells you how far off _the right order of magnitude_ you are when trying to reproduce each observation. This affects what our model most cares about getting right.\n",
    "\n",
    "The mean square difference cares most about getting higher losses right, since a $1,000 error is equally important for $10 k event as for a $10 bn event.\n",
    "\n",
    "The mean square log difference cares most about getting events about right. Since is looks at logarithms of the values, a 50% error is equally important for a $10 k event as for a $10 bn event.\n",
    "\n",
    "In the first calibrations here, we take the latter approach, hoping to get the order of magnitude about right. (Thinking about it, I think the former approach might be more justifiable: the main disruption to supply chains is likely to come from really big events, and maybe we want to focus on getting them right!)\n",
    "\n",
    "## V1 calibrated components\n",
    "\n",
    "Each of these components is calibrated separately against the EM-DAT observations we have:\n",
    "\n",
    "- Tropical cyclone asset loss against EM-DAT\n",
    "- River flood asset loss against EM-DAT\n",
    "- European windstorm asset loss against EM-DAT\n",
    "\n",
    "There is some regional varition in the calibration. For both the tropical cyclone and river flood impact functions the globe is split into regions, and impact functions are fit independently for each region (and the European windstorm is already regional). This is improved on in Version 2. \n",
    "\n",
    "\n",
    "### Still to be solved:\n",
    "\n",
    "- Relative crop yield (need observation data)\n",
    "- If we have other loss observations beyond EM-DAT we could extend the calibration to include them\n",
    "- Some form of validation: likely a cross-validation\n",
    "\n",
    "\n",
    "### Notes\n",
    "\n",
    "- While in this example, the observations for validation are all single-event losses, the observations that we're validating against are allowed to be very heterogeneous. As long as it can be calculated from the model output and evaluated with a cost function, it can be compared to an observation. That means that we could add summary stats, e.g. expected annual impacts, from other studies. The methodology is flexible.\n",
    "- There are sufficiently many observations here that our choice of priors aren't too important\n",
    "- Wildfire is now no longer part of the NCCS study and isn't included in the modelling or calibration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version 2: add regional vulnerability to direct asset losses\n",
    "\n",
    "Version 2 is similar to Version 1, but adds an extra degree of freedom to each calibrated impact function in the form of a vulnerability parameter. This is (proposed) to be taken from the ND-GAIN data that is already used by EBP in their metrics, since it's trusted by our group and ensures a consisten idea of 'vulnerability' between components of the project.\n",
    "\n",
    "The calibration will assign a vulnerability index to each country and allow the vulnerability curves to change based on vulnerability. (Exactly what we use will take a little experimentation: should it be the raw ND-GAIN index, the country rank, quantile information, or something else? And which part of the impact function should it change? We can run a few quick tests.)\n",
    "\n",
    "There are two challenges from  adding an extra degree of freedom:\n",
    "\n",
    "- **Computational complexity:** as the parameter space grows in dimensionality, it takes longer for the algorithm to explore it and find the optimal parameter combinations\n",
    "- **Parameter uncertainty:** for each additional parameter you add to a statistical model, there is a tradeoff between the explanatory power it provides versus the increasing uncertainty in each fitted parametr, especially in models trained on small datasetslike ours. Our hope is that vulnerability adds so much explanatory power that there is little additional uncertainty. It's a reasonable hope, since we know losses depend a lot on a country's vulnerability, and we'll be able to train the function on more data since we won't be fitting independent functions for different world regions. We'll be able to compare the V1 and V2 calibrations and decide where the vulnerability information improves the model.\n",
    "\n",
    "### V2 calibration components\n",
    "These are the same as the V1 components\n",
    "\n",
    "### Still to be solved\n",
    "\n",
    "- The best way to represent the effect of national vulnerability in a family of impact functions, discussed above.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version 3: add expert judgement (and any observations) for uncalibrated components\n",
    "\n",
    "Until now, the calibration has only focussed on calibrating the asset loss calculations. We now look at the Business Interruption componenet. This section looks only at the calculations of BI from asset losses, not from hazards:\n",
    "\n",
    "<img src=\"images/calibration_component_BI.png\" width=\"800\" alt=\"The NCCS modelling pipeline with the production loss component highlighted\">\n",
    "\n",
    "Currently we have very few observations of business interruption, meaning that this calibration won't change much. We can use some expert judgement to specify our priors (some confidence interval around the Hazus values), and that might be as far as we can go.\n",
    "\n",
    "### V3 calibration components\n",
    "\n",
    "### Still to be solved\n",
    "\n",
    "- How to describe perturbations to the (empirical) functions built by Hazus? Most likely a simple scaling on the y-axis\n",
    "- How to describe the effect of regional vulnerability, if at all\n",
    "\n",
    "Why might we decide not to include regional vulnerability? The main reason is that we have so few observations from outside the US. The theoretical justification is that regional vulnerabilities are already part of the asset loss calculation: more vulnerable countries see (on average) higher losses from the same hazard when compared to less vulnerable countries, and this translates into higher business interruption. So we could justify the assumption that no additional vulnerability component is needed in the BI calculations. This is unlikely to be the case, and we will have to inspect the available and modelled data to see if it holds up.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Version 4: calibrate entire modelling pipeline\n",
    "\n",
    "Version 4 is the big conceptual shift. Instead of calibrating individual components we calibrate multiple components at once:\n",
    "\n",
    "<img src=\"images/calibration_whole_chain.png\" width=\"800\" alt=\"The NCCS modelling pipeline for a whole-pipeline calibration\">\n",
    "\n",
    "(This diagram doesn't include the right hand side of the modelling chain because I'm not expecting to have observations for it. It would be easy, though not cheap, to extend this.)\n",
    "\n",
    "While calibrating individual components is good, it doesn't guarantee that we have a good calibration when we combine them into a single pipeline. This is especially true in the NCCS chain where the production loss impact curves are produced from HAZUS data which was calibrated against different hazard data to the CLIMADA asset loss data. That means that if, for example, the CLIMADA hazard has a low bias compared to the hazard used in HAZUS calibrations, it will propagate through the production loss chain, giving low biases to the modelled production loss, even though each component was calibrated.\n",
    "\n",
    "The purpose of a whole-chain calibration is to fix these biases, adjusting all parameters simultaneously. (Assuming, however, that we have observations for the whole chain, i.e. relating hazard to business interruption – even if it's just return-period BI.)\n",
    "\n",
    "### Notes\n",
    "\n",
    "- We would probably use narrower priors in this calibration, given the computational complexity. The adjustments here are (hopefully) just to adjust for biases, rather than find completely different solutions to the ones found in the calibration of the individual components."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
